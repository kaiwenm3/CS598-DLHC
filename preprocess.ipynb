{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPpRG+vHhmmMuOikr0q5S29"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bmGQ5HvRf972","executionInfo":{"status":"ok","timestamp":1746672359859,"user_tz":300,"elapsed":24140,"user":{"displayName":"Kaiwen Min","userId":"03847312882102955198"}},"outputId":"bd2a1301-f2e7-468a-801b-f6da78cef7a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install poetry"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VXE8gWmGgLnO","executionInfo":{"status":"ok","timestamp":1746667654812,"user_tz":300,"elapsed":14342,"user":{"displayName":"Kaiwen Min","userId":"03847312882102955198"}},"outputId":"7101a525-e7b1-458b-dfb3-3d508df12eab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting poetry\n","  Downloading poetry-2.1.3-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: build<2.0.0,>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from poetry) (1.2.2.post1)\n","Requirement already satisfied: cachecontrol<0.15.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (0.14.3)\n","Collecting cleo<3.0.0,>=2.1.0 (from poetry)\n","  Downloading cleo-2.1.0-py3-none-any.whl.metadata (12 kB)\n","Collecting dulwich<0.23.0,>=0.22.6 (from poetry)\n","  Downloading dulwich-0.22.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: fastjsonschema<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from poetry) (2.21.1)\n","Collecting findpython<0.7.0,>=0.6.2 (from poetry)\n","  Downloading findpython-0.6.3-py3-none-any.whl.metadata (5.3 kB)\n","Collecting installer<0.8.0,>=0.7.0 (from poetry)\n","  Downloading installer-0.7.0-py3-none-any.whl.metadata (936 bytes)\n","Requirement already satisfied: keyring<26.0.0,>=25.1.0 in /usr/local/lib/python3.11/dist-packages (from poetry) (25.6.0)\n","Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.11/dist-packages (from poetry) (24.2)\n","Collecting pbs-installer<2026.0.0,>=2025.1.6 (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry)\n","  Downloading pbs_installer-2025.4.9-py3-none-any.whl.metadata (990 bytes)\n","Collecting pkginfo<2.0,>=1.12 (from poetry)\n","  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: platformdirs<5,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from poetry) (4.3.7)\n","Collecting poetry-core==2.1.3 (from poetry)\n","  Downloading poetry_core-2.1.3-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: pyproject-hooks<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from poetry) (1.2.0)\n","Requirement already satisfied: requests<3.0,>=2.26 in /usr/local/lib/python3.11/dist-packages (from poetry) (2.32.3)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from poetry) (1.0.0)\n","Requirement already satisfied: shellingham<2.0,>=1.5 in /usr/local/lib/python3.11/dist-packages (from poetry) (1.5.4)\n","Collecting tomlkit<1.0.0,>=0.11.4 (from poetry)\n","  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n","Collecting trove-classifiers>=2022.5.19 (from poetry)\n","  Downloading trove_classifiers-2025.5.7.19-py3-none-any.whl.metadata (2.3 kB)\n","Collecting virtualenv<21.0.0,>=20.26.6 (from poetry)\n","  Downloading virtualenv-20.31.1-py3-none-any.whl.metadata (4.5 kB)\n","Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from cachecontrol<0.15.0,>=0.14.0->cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (1.1.0)\n","Requirement already satisfied: filelock>=3.8.0 in /usr/local/lib/python3.11/dist-packages (from cachecontrol[filecache]<0.15.0,>=0.14.0->poetry) (3.18.0)\n","Collecting crashtest<0.5.0,>=0.4.1 (from cleo<3.0.0,>=2.1.0->poetry)\n","  Downloading crashtest-0.4.1-py3-none-any.whl.metadata (1.1 kB)\n","Collecting rapidfuzz<4.0.0,>=3.0.0 (from cleo<3.0.0,>=2.1.0->poetry)\n","  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from dulwich<0.23.0,>=0.22.6->poetry) (2.4.0)\n","Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.3.3)\n","Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (0.9.0)\n","Requirement already satisfied: importlib_metadata>=4.11.4 in /usr/local/lib/python3.11/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (8.7.0)\n","Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (3.4.0)\n","Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (4.1.0)\n","Requirement already satisfied: jaraco.context in /usr/local/lib/python3.11/dist-packages (from keyring<26.0.0,>=25.1.0->poetry) (6.0.1)\n","Requirement already satisfied: httpx<1,>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.28.1)\n","Requirement already satisfied: zstandard>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.23.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.26->poetry) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.26->poetry) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.26->poetry) (2025.4.26)\n","Collecting distlib<1,>=0.3.7 (from virtualenv<21.0.0,>=20.26.6->poetry)\n","  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (4.9.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (0.16.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.11.4->keyring<26.0.0,>=25.1.0->poetry) (3.21.0)\n","Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.11/dist-packages (from SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (43.0.3)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring<26.0.0,>=25.1.0->poetry) (10.7.0)\n","Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.11/dist-packages (from jaraco.context->keyring<26.0.0,>=25.1.0->poetry) (1.2.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (1.17.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (1.3.1)\n","Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.27.0->pbs-installer[download,install]<2026.0.0,>=2025.1.6->poetry) (4.13.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.0->SecretStorage>=3.2->keyring<26.0.0,>=25.1.0->poetry) (2.22)\n","Downloading poetry-2.1.3-py3-none-any.whl (278 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading poetry_core-2.1.3-py3-none-any.whl (332 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m332.6/332.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cleo-2.1.0-py3-none-any.whl (78 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dulwich-0.22.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading findpython-0.6.3-py3-none-any.whl (20 kB)\n","Downloading installer-0.7.0-py3-none-any.whl (453 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pbs_installer-2025.4.9-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n","Downloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n","Downloading trove_classifiers-2025.5.7.19-py3-none-any.whl (14 kB)\n","Downloading virtualenv-20.31.1-py3-none-any.whl (6.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading crashtest-0.4.1-py3-none-any.whl (7.6 kB)\n","Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trove-classifiers, distlib, virtualenv, tomlkit, rapidfuzz, poetry-core, pkginfo, pbs-installer, installer, findpython, dulwich, crashtest, cleo, poetry\n","Successfully installed cleo-2.1.0 crashtest-0.4.1 distlib-0.3.9 dulwich-0.22.8 findpython-0.6.3 installer-0.7.0 pbs-installer-2025.4.9 pkginfo-1.12.1.2 poetry-2.1.3 poetry-core-2.1.3 rapidfuzz-3.13.0 tomlkit-0.13.2 trove-classifiers-2025.5.7.19 virtualenv-20.31.1\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/My Drive/cs598_project/preparation')"],"metadata":{"id":"UDor0E2NgUud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install wfdb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJ9ADKj-h7MU","executionInfo":{"status":"ok","timestamp":1746672377663,"user_tz":300,"elapsed":11061,"user":{"displayName":"Kaiwen Min","userId":"03847312882102955198"}},"outputId":"9b388994-a1fd-4298-f65d-36f5045f690c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wfdb\n","  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.11.15)\n","Requirement already satisfied: fsspec>=2023.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2025.3.2)\n","Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n","Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.0.2)\n","Collecting pandas>=2.2.3 (from wfdb)\n","  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n","Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.15.2)\n","Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.10.11->wfdb) (1.20.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.4.26)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n","Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pandas, wfdb\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.2.2\n","    Uninstalling pandas-2.2.2:\n","      Successfully uninstalled pandas-2.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pandas-2.2.3 wfdb-4.3.0\n"]}]},{"cell_type":"code","source":["!pip install iterative-stratification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iCNzippFiXb3","executionInfo":{"status":"ok","timestamp":1746672384933,"user_tz":300,"elapsed":7268,"user":{"displayName":"Kaiwen Min","userId":"03847312882102955198"}},"outputId":"8f96339a-4b29-4351-d27f-b2d90175632b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting iterative-stratification\n","  Downloading iterative_stratification-0.1.9-py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.15.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from iterative-stratification) (1.6.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->iterative-stratification) (3.6.0)\n","Downloading iterative_stratification-0.1.9-py3-none-any.whl (8.5 kB)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.9\n"]}]},{"cell_type":"code","source":["import os\n","import sys\n","from collections import Counter\n","from typing import List, Tuple, Type\n","from glob import glob\n","\n","import wfdb\n","import numpy as np\n","from tqdm import tqdm\n","from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n","\n","sys.path.append(\"..\")\n","import config\n","import utils\n","\n","class G12ECPreparator:\n","\n","    def __init__(self,\n","        sampling_frequency: int=500,\n","        split_number: int=1,\n","        min_sample_ratio: float=0.01,\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            sampling_frequency (int): Sampling frequency (500).\n","            split_number (int): Seed　value for train test split.\n","            min_samples (float): Number of minimum sample ratio.\n","        \"\"\"\n","\n","        self.sampling_frequency = sampling_frequency\n","        self.split_number = split_number\n","        self.min_sample_ratio = min_sample_ratio\n","\n","        self.load_dir = os.path.join(\n","            '/content/drive/My Drive/cs598_project/data', config.dirname_g12ec, \"WFDB\")\n","        self.save_dir = os.path.join(\n","            '/content/drive/My Drive/cs598_project/data', config.dirname_g12ec, \"processed\")\n","        os.makedirs(self.save_dir, exist_ok=True)\n","\n","    def _open_heafile(self, hea_file: str) -> Type[wfdb.io.record.Record]:\n","        \"\"\"\n","        Args:\n","            hea_file (str): Path to hea file.\n","        Returns:\n","            waveform_data ():\n","        \"\"\"\n","        basename, _ = os.path.splitext(hea_file)\n","        waveform_data = wfdb.rdrecord(basename)\n","        return waveform_data\n","\n","    def _load_data(self) -> Tuple[np.ndarray, List, Tuple[List, List]]:\n","        \"\"\"\n","        Args:\n","            None\n","        Returns:\n","            signals (np.ndarray): Array of 12lead ECG signals with length num_samples.\n","                            (Each elements are array of shape [sequence_length, 12])\n","            dxs (List): List of diagnosis ids\n","            demographics (Tuple): Tuple of list of sex and age of each data.\n","        \"\"\"\n","        hea_files = sorted(glob(self.load_dir + \"/*.hea\"))\n","        print(f\"Found {len(hea_files)} files.\")\n","        signals = []\n","        dxs, sexs, ages = [], [], []\n","        for hea_file in tqdm(hea_files):\n","            print(hea_file)\n","            data = self._open_heafile(hea_file)\n","            assert(data.n_sig == 12)\n","            assert(data.fs == 500)\n","            assert(data.sig_name == config.g12ec_lead_order)\n","            signal = np.nan_to_num(data.p_signal, 0)\n","            signals.append(signal)\n","\n","            sexs.append(data.comments[1])\n","            ages.append(data.comments[0])\n","            dxs.append(data.comments[2])\n","        return np.array(signals, dtype=object), dxs, (sexs, ages)\n","\n","    def _align_signal_length(self, signals: np.ndarray):\n","        \"\"\"\n","        Args:\n","            signals (np.ndarray):\n","        Returns:\n","            aligned_signals (np.ndarray):\n","        \"\"\"\n","        aligned_signals = []\n","        for signal in signals:\n","            # Padding\n","            signal_length = signal.shape[0]\n","            if signal_length > config.g12ec_default_signal_length:\n","                raise ValueError(f\"Signal length {signal_length} exceeded default_signal_length.\")\n","            elif signal_length < config.g12ec_default_signal_length:\n","                pad_length = config.g12ec_default_signal_length - signal_length\n","                pad = np.zeros([pad_length, signal.shape[1]])\n","                signal = np.concatenate([pad, signal], axis=0)\n","            aligned_signals.append(signal)\n","        aligned_signals = np.stack(aligned_signals)\n","        return aligned_signals\n","\n","    def _preprocess_signal(\n","        self,\n","        X_train: np.ndarray,\n","        X_val: np.ndarray,\n","        X_test: np.ndarray\n","    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","        \"\"\"\n","        Scale data.\n","\n","        Args:\n","            X_train (np.ndarray): Array of arrays of shape [(sequence_length, 12), (..), .., (..)].\n","            X_val (np.ndarray): Array of arrays of shape [(sequence_length, 12), (..), .., (..)].\n","            X_test (np.ndarray): Array of arrays of shape [(sequence_length, 12), (..), .., (..)].\n","        Returns:\n","            X_train (np.ndarray): Array of arrays of shape [(sequence_length, 12), (..), .., (..)].\n","            X_val (np.ndarray): Array of arrays of shape [(sequence_length, 12), (..), .., (..)].\n","            X_test (np.ndarray): Array of arrays of shape [(sequence_length, 12), (..), .., (..)].\n","        \"\"\"\n","        # apply padding\n","        X_train = self._align_signal_length(X_train)\n","        X_val = self._align_signal_length(X_val)\n","        X_test = self._align_signal_length(X_test)\n","\n","        # apply scaling\n","        X_train, X_val, X_test = utils.preprocess_signals(\n","            X_train, X_val, X_test, self.save_dir, seed=self.split_number)\n","\n","        return X_train, X_val, X_test\n","\n","    def _process_label(self, labels: List) -> Tuple[np.ndarray, np.ndarray]:\n","        \"\"\"\n","        Args:\n","            labels (List):\n","        Returns:\n","            processed_labels (np.ndarray): Array of shape (num_samples, num_labels).\n","            target_labels (np.ndarray): List of dx code corresponding to `processed_labels` index.\n","        \"\"\"\n","        label_index = []\n","        for label in labels:\n","            # label = \"Dx: XXXXX,YYYYY\"\n","            label = label.replace(\"Dx: \", \"\")\n","            label = label.split(\",\")\n","            label_index += label\n","\n","        # Select labels with more than `self.min_sample_ratio * len(labels)`.\n","        target_labels = []\n","        for idx, count in Counter(label_index).items():\n","            if count > int(self.min_sample_ratio * len(labels)):\n","                target_labels.append(idx)\n","\n","        processed_labels = np.zeros([len(labels), len(target_labels)])\n","        for i, label in enumerate(labels):\n","            # label = \"Dx: XXXXX,YYYYY\"\n","            label = label.replace(\"Dx: \", \"\")\n","            label = label.split(\",\")\n","            for l in label:\n","                if l in target_labels:\n","                    processed_labels[i, target_labels.index(l)] = 1\n","        return processed_labels, np.array(target_labels)\n","\n","    def _process_demographics(self, demographics: Tuple):\n","        \"\"\"\n","        Args:\n","            demographics (Tuple[np.ndarray]):\n","        Returns:\n","            processed_demos (np.ndarray):\n","        \"\"\"\n","        processed_demos = []\n","        sexs, ages = demographics\n","        for (sex, age) in zip(sexs, ages):\n","            sex = sex.lower().replace(\"sex: \", \"\")\n","            assert(sex in [\"male\", \"female\"])\n","            sex = int(sex == \"male\")\n","\n","            age = age.lower().replace(\"age: \", \"\")\n","            age = int(age) if age.isdigit() else np.nan\n","            processed_demos.append([age, sex])\n","        processed_demos = np.array(processed_demos)\n","        return processed_demos\n","\n","    def _split_data(\n","        self,\n","        data: np.ndarray,\n","        labels: np.ndarray,\n","    ) -> Tuple[Tuple, Tuple, Tuple]:\n","        \"\"\"\n","        Args:\n","            data (np.ndarray): Array of shape (num_samples, ).\n","            labels (np.ndarray): Array of shape (num_samples, num_classes)\n","            demographics (np.ndarray): Tuple of shape (num_samples, 2 (age, sex)).\n","        Returns:\n","            train_data (Tuple):\n","            valid_data (Tuple):\n","            test_data (Tuple):\n","        \"\"\"\n","        msss_1 = MultilabelStratifiedShuffleSplit(\n","            n_splits=1, test_size=0.2, random_state=self.split_number)\n","\n","        for train_idx, test_idx in msss_1.split(data, labels):\n","            pass\n","\n","        msss_2 = MultilabelStratifiedShuffleSplit(\n","            n_splits=1, test_size=0.5, random_state=self.split_number)\n","\n","        for valid_idx, test_idx in msss_2.split(data[test_idx], labels[test_idx]):\n","            pass\n","\n","        X_train, X_valid, X_test =\\\n","            data[train_idx], data[valid_idx], data[test_idx]\n","        y_train, y_valid, y_test =\\\n","            labels[train_idx], labels[valid_idx], labels[test_idx]\n","\n","        return (X_train, y_train), (X_valid, y_valid), (X_test, y_test)\n","\n","    def _dump_data(self, X: np.ndarray, y: np.ndarray, datatype: int) -> None:\n","        \"\"\"\n","        Args:\n","            X (np.ndarray):\n","            y (np.ndarray):\n","            datatype (str):\n","        Returns:\n","            None\n","        \"\"\"\n","        print(f\"Saving {datatype} data ...\")\n","        X.dump(self.save_dir + f'/X_{datatype}_seed{self.split_number}.npy', protocol=4)\n","        y.dump(self.save_dir + f'/y_{datatype}_seed{self.split_number}.npy', protocol=4)\n","\n","    def prepare(self):\n","        \"\"\"\n","        Args:\n","\n","        Returns:\n","\n","        \"\"\"\n","        # Load G12EC data\n","        signals, dxs, demographics = self._load_data()\n","\n","        print(signals)\n","\n","        processed_labels, label_index = self._process_label(dxs)\n","        processed_demos = self._process_demographics(demographics)\n","\n","        # Split data into train, valid, test\n","        (X_train, y_train), (X_val, y_val), (X_test, y_test) =\\\n","            self._split_data(signals, processed_labels)\n","\n","        X_train, X_val, X_test = self._preprocess_signal(X_train, X_val, X_test)\n","\n","        self._dump_data(X_train, y_train, \"train\")\n","        self._dump_data(X_val, y_val, \"val\")\n","        self._dump_data(X_test, y_test, \"test\")\n","        label_index.dump(self.save_dir + \"/label_index.npy\")\n","\n","\n"],"metadata":{"id":"VBI93R6Rgdj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    import sys\n","\n","    sampling_frequency = 500\n","    # split_number = int(sys.argv[1])\n","    for split_number in range(1, 6):\n","        print(f\"Working on split_number: {split_number} ...\")\n","        preparator = G12ECPreparator(sampling_frequency, split_number)\n","        preparator.prepare()"],"metadata":{"id":"hCkFL5tNpEgx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Based on code from `https://github.com/helme/ecg_ptbxl_benchmarking`\n","`master/code/experiments/scp_experiment.py`\n","\"\"\"\n","import os\n","import sys\n","import pickle\n","\n","import numpy as np\n","\n","sys.path.append(\"..\")\n","import config\n","import utils\n","\n","class DataPreparator():\n","\n","    folds_type='strat'\n","\n","    def __init__(\n","        self,\n","        task: str,\n","        min_samples: int,\n","        sampling_frequency: int,\n","        split_number: int=1\n","    ) -> None:\n","        \"\"\"\n","        Args:\n","            task (str): Name of task ('all', 'diagnostic', 'subdiagnostic',\n","                                      'superdiagnostic', 'form', 'rhythm')\n","            min_samples (int):\n","            sampling_frequency (int): Sampling frequency (100 or 500).\n","            split_number (int): Select val and test fold index.\n","                val_fold_index (int): Index of stratifed split for validation dataset.\n","                test_fold_index (int): Index of stratifed split for test dataset.\n","                    (Other 8 indices not used will be treated as train_fold_indices)\n","        \"\"\"\n","\n","        assert(task in config.TASKS)\n","        self.task = task\n","        self.min_samples = min_samples\n","        self.sampling_frequency = sampling_frequency\n","\n","        self.val_fold_index = config.split_settings[split_number][\"val_index\"]\n","        self.test_fold_index = config.split_settings[split_number][\"test_index\"]\n","        setting = f\"{task}/val-{self.val_fold_index}_test-{self.test_fold_index}/\"\n","\n","        self.load_dir = os.path.join(config.data_root, config.dirname_ptbxl, \"raw\")\n","        self.save_dir = os.path.join(config.data_root, config.dirname_ptbxl, setting)\n","        os.makedirs(self.save_dir, exist_ok=True)\n","\n","    def _split_data(self, data: np.ndarray, labels: np.ndarray, y_data: np.ndarray):\n","        \"\"\"\n","        Args:\n","            data (np.ndarray): Array of shape (num_samples, sequence_length, 12).\n","            labels (np.ndarray): Array of shape (num_samples, ??)\n","            Y (np.ndarray): Array of shape (num_samples, ??)\n","        Returns:\n","\n","        \"\"\"\n","        test_target = labels.strat_fold == self.test_fold_index\n","        X_test = data[test_target]\n","        y_test = y_data[test_target]\n","\n","        val_target = labels.strat_fold == self.val_fold_index\n","        X_val = data[val_target]\n","        y_val = y_data[val_target]\n","\n","        train_target = ~val_target & ~test_target\n","        X_train = data[train_target]\n","        y_train = y_data[train_target]\n","        return (X_train, y_train), (X_val, y_val), (X_test, y_test)\n","\n","    def _dump_data(self, X: np.ndarray, y: np.ndarray, datatype: str) -> None:\n","        \"\"\"\n","        Args:\n","            X (np.ndarray):\n","            y (np.ndarray):\n","            datatype (str):\n","        Returns:\n","            None\n","        \"\"\"\n","        print(f\"Saving {datatype} data ...\")\n","        X.dump(self.save_dir + f'X_{datatype}.npy', protocol=4)\n","        y.dump(self.save_dir + f'y_{datatype}.npy', protocol=4)\n","\n","    def prepare(self):\n","        \"\"\"\n","        Args:\n","        Returns:\n","        \"\"\"\n","        # Load PTB-XL data\n","        print(self.load_dir)\n","        data, raw_labels = load_dataset(\n","            self.load_dir, self.sampling_frequency)\n","\n","        # Preprocess label data\n","        labels = utils.compute_label_aggregations(\n","            raw_labels, self.load_dir, self.task)\n","\n","        # Select relevant data and convert to one-hot\n","        data, labels, Y, _ = utils.select_data(\n","            data, labels, self.task, self.min_samples, self.save_dir)\n","\n","        # Split data into train, valid, test\n","        (X_train, y_train), (X_val, y_val), (X_test, y_test) =\\\n","            self._split_data(data, labels, Y)\n","\n","\n","        X_train, X_val, X_test = utils.preprocess_signals(\n","            X_train, X_val, X_test, self.save_dir)\n","\n","        self._dump_data(X_train, y_train, \"train\")\n","        self._dump_data(X_val, y_val, \"val\")\n","        self._dump_data(X_test, y_test, \"test\")\n","\n","if __name__ == \"__main__\":\n","\n","    min_samples = 0\n","    sampling_frequency = 500\n","    split_number = 1\n","    for task in config.TASKS:\n","        print(f\"Working on {task} data (split_number: {split_number})...\")\n","        preparator = DataPreparator(task, min_samples, sampling_frequency,\n","                                    split_number)\n","        preparator.prepare()\n"],"metadata":{"id":"XR-sWOcunUaL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import sys\n","import ast\n","import pickle\n","\n","import wfdb\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer\n","\n","sys.path.append(\"..\")\n","import config\n","\n","def form_datasplit_string(split_number: int) -> str:\n","    \"\"\"\n","    Form string containing datasplit information (eg. `val-9_test-10`)\n","    Args:\n","        split_number (int):\n","    Returns:\n","        data_split_string (str):\n","    \"\"\"\n","    fold_indices = config.split_settings[split_number]\n","    val_fold_index = fold_indices[\"val_index\"]\n","    test_fold_index = fold_indices[\"test_index\"]\n","    data_split_string = f\"val-{val_fold_index}_test-{test_fold_index}\"\n","    return data_split_string\n","\n","def load_dataset(path, sampling_rate, release=False):\n","    \"\"\"\n","    Returns:\n","        X (np.ndarray):\n","        Y (pd.DataFrame):\n","    \"\"\"\n","\n","    if path.split('/')[-2] == 'PTBXL':\n","        # load and convert annotation data\n","        Y = pd.read_csv(path+'/ptbxl_database.csv', index_col='ecg_id')\n","        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n","\n","        # Load raw signal data\n","        X = load_raw_data_ptbxl(Y, sampling_rate, path)\n","\n","    elif path.split('/')[-2] == 'ICBEB':\n","        # load and convert annotation data\n","        Y = pd.read_csv(path+'/icbeb_database.csv', index_col='ecg_id')\n","        Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))\n","\n","        # Load raw signal data\n","        X = load_raw_data_icbeb(Y, sampling_rate, path)\n","\n","    return X, Y\n","\n","def load_raw_data_ptbxl(df, sampling_rate, path):\n","    if sampling_rate == 100:\n","        if os.path.exists(path + '/raw100.npy'):\n","            data = np.load(path+'/raw100.npy', allow_pickle=True)\n","        else:\n","            data = [wfdb.rdsamp(path+'/'+f) for f in tqdm(df.filename_lr)]\n","            data = np.array([signal for signal, meta in data])\n","            pickle.dump(data, open(path+'/raw100.npy', 'wb'), protocol=4)\n","    elif sampling_rate == 500:\n","        if os.path.exists(path + '/raw500.npy'):\n","            data = np.load(path+'/raw500.npy', allow_pickle=True)\n","        else:\n","            data = [wfdb.rdsamp(path+'/'+f) for f in tqdm(df.filename_hr)]\n","            data = np.array([signal for signal, meta in data])\n","            pickle.dump(data, open(path+'/raw500.npy', 'wb'), protocol=4)\n","    return data\n","\n","def compute_label_aggregations(df, folder, ctype):\n","\n","    df['scp_codes_len'] = df.scp_codes.apply(lambda x: len(x))\n","\n","    aggregation_df = pd.read_csv(folder+'/scp_statements.csv', index_col=0)\n","\n","    if ctype in ['diagnostic', 'subdiagnostic', 'superdiagnostic']:\n","\n","        def aggregate_all_diagnostic(y_dic):\n","            tmp = []\n","            for key in y_dic.keys():\n","                if key in diag_agg_df.index:\n","                    tmp.append(key)\n","            return list(set(tmp))\n","\n","        def aggregate_subdiagnostic(y_dic):\n","            tmp = []\n","            for key in y_dic.keys():\n","                if key in diag_agg_df.index:\n","                    c = diag_agg_df.loc[key].diagnostic_subclass\n","                    if str(c) != 'nan':\n","                        tmp.append(c)\n","            return list(set(tmp))\n","\n","        def aggregate_diagnostic(y_dic):\n","            tmp = []\n","            for key in y_dic.keys():\n","                if key in diag_agg_df.index:\n","                    c = diag_agg_df.loc[key].diagnostic_class\n","                    if str(c) != 'nan':\n","                        tmp.append(c)\n","            return list(set(tmp))\n","\n","        diag_agg_df = aggregation_df[aggregation_df.diagnostic == 1.0]\n","        if ctype == 'diagnostic':\n","            df['diagnostic'] = df.scp_codes.apply(aggregate_all_diagnostic)\n","            df['diagnostic_len'] = df.diagnostic.apply(lambda x: len(x))\n","        elif ctype == 'subdiagnostic':\n","            df['subdiagnostic'] = df.scp_codes.apply(aggregate_subdiagnostic)\n","            df['subdiagnostic_len'] = df.subdiagnostic.apply(lambda x: len(x))\n","        elif ctype == 'superdiagnostic':\n","            df['superdiagnostic'] = df.scp_codes.apply(aggregate_diagnostic)\n","            df['superdiagnostic_len'] = df.superdiagnostic.apply(lambda x: len(x))\n","    elif ctype == 'form':\n","        form_agg_df = aggregation_df[aggregation_df.form == 1.0]\n","\n","        def aggregate_form(y_dic):\n","            tmp = []\n","            for key in y_dic.keys():\n","                if key in form_agg_df.index:\n","                    c = key\n","                    if str(c) != 'nan':\n","                        tmp.append(c)\n","            return list(set(tmp))\n","\n","        df['form'] = df.scp_codes.apply(aggregate_form)\n","        df['form_len'] = df.form.apply(lambda x: len(x))\n","    elif ctype == 'rhythm':\n","        rhythm_agg_df = aggregation_df[aggregation_df.rhythm == 1.0]\n","\n","        def aggregate_rhythm(y_dic):\n","            tmp = []\n","            for key in y_dic.keys():\n","                if key in rhythm_agg_df.index:\n","                    c = key\n","                    if str(c) != 'nan':\n","                        tmp.append(c)\n","            return list(set(tmp))\n","\n","        df['rhythm'] = df.scp_codes.apply(aggregate_rhythm)\n","        df['rhythm_len'] = df.rhythm.apply(lambda x: len(x))\n","    elif ctype == 'all':\n","        df['all_scp'] = df.scp_codes.apply(lambda x: list(set(x.keys())))\n","\n","    return df\n","\n","def select_data(XX, YY, ctype, min_samples, outputfolder):\n","    # convert multilabel to multi-hot\n","    mlb = MultiLabelBinarizer()\n","\n","    if ctype == 'diagnostic':\n","        X = XX[YY.diagnostic_len > 0]\n","        Y = YY[YY.diagnostic_len > 0]\n","        mlb.fit(Y.diagnostic.values)\n","        y = mlb.transform(Y.diagnostic.values)\n","    elif ctype == 'subdiagnostic':\n","        counts = pd.Series(np.concatenate(YY.subdiagnostic.values)).value_counts()\n","        counts = counts[counts > min_samples]\n","        YY.subdiagnostic = YY.subdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n","        YY['subdiagnostic_len'] = YY.subdiagnostic.apply(lambda x: len(x))\n","        X = XX[YY.subdiagnostic_len > 0]\n","        Y = YY[YY.subdiagnostic_len > 0]\n","        mlb.fit(Y.subdiagnostic.values)\n","        y = mlb.transform(Y.subdiagnostic.values)\n","    elif ctype == 'superdiagnostic':\n","        counts = pd.Series(np.concatenate(YY.superdiagnostic.values)).value_counts()\n","        counts = counts[counts > min_samples]\n","        YY.superdiagnostic = YY.superdiagnostic.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n","        YY['superdiagnostic_len'] = YY.superdiagnostic.apply(lambda x: len(x))\n","        X = XX[YY.superdiagnostic_len > 0]\n","        Y = YY[YY.superdiagnostic_len > 0]\n","        mlb.fit(Y.superdiagnostic.values)\n","        y = mlb.transform(Y.superdiagnostic.values)\n","    elif ctype == 'form':\n","        # filter\n","        counts = pd.Series(np.concatenate(YY.form.values)).value_counts()\n","        counts = counts[counts > min_samples]\n","        YY.form = YY.form.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n","        YY['form_len'] = YY.form.apply(lambda x: len(x))\n","        # select\n","        X = XX[YY.form_len > 0]\n","        Y = YY[YY.form_len > 0]\n","        mlb.fit(Y.form.values)\n","        y = mlb.transform(Y.form.values)\n","    elif ctype == 'rhythm':\n","        # filter\n","        counts = pd.Series(np.concatenate(YY.rhythm.values)).value_counts()\n","        counts = counts[counts > min_samples]\n","        YY.rhythm = YY.rhythm.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n","        YY['rhythm_len'] = YY.rhythm.apply(lambda x: len(x))\n","        # select\n","        X = XX[YY.rhythm_len > 0]\n","        Y = YY[YY.rhythm_len > 0]\n","        mlb.fit(Y.rhythm.values)\n","        y = mlb.transform(Y.rhythm.values)\n","    elif ctype == 'all':\n","        # filter\n","        counts = pd.Series(np.concatenate(YY.all_scp.values)).value_counts()\n","        counts = counts[counts > min_samples]\n","        YY.all_scp = YY.all_scp.apply(lambda x: list(set(x).intersection(set(counts.index.values))))\n","        YY['all_scp_len'] = YY.all_scp.apply(lambda x: len(x))\n","        # select\n","        X = XX[YY.all_scp_len > 0]\n","        Y = YY[YY.all_scp_len > 0]\n","        mlb.fit(Y.all_scp.values)\n","        y = mlb.transform(Y.all_scp.values)\n","    else:\n","        pass\n","\n","    # save LabelBinarizer\n","    with open(outputfolder+'mlb.pkl', 'wb') as tokenizer:\n","        pickle.dump(mlb, tokenizer)\n","\n","    return X, Y, y, mlb\n","\n","def preprocess_signals(X_train, X_validation, X_test, outputfolder, seed=None):\n","    # Standardize data such that mean 0 and variance 1\n","    ss = StandardScaler()\n","    ss.fit(np.vstack(X_train).flatten()[:,np.newaxis].astype(float))\n","\n","    # Save Standardizer data\n","    if seed is None: # For PTB-XL dataset\n","        filename = '/standard_scaler.pkl'\n","    else:\n","        filename = f'/standard_scaler_seed{seed}.pkl'\n","\n","    with open(outputfolder+filename, 'wb') as ss_file:\n","        pickle.dump(ss, ss_file)\n","\n","    X_train = apply_standardizer(X_train, ss)\n","    X_valid = apply_standardizer(X_validation, ss)\n","    X_test = apply_standardizer(X_test, ss)\n","    return X_train, X_valid, X_test\n","\n","def apply_standardizer(X, ss):\n","    X_tmp = []\n","    for x in X:\n","        x_shape = x.shape\n","        X_tmp.append(ss.transform(x.flatten()[:,np.newaxis]).reshape(x_shape))\n","    X_tmp = np.array(X_tmp, dtype='object')\n","    return X_tmp\n"],"metadata":{"id":"HEPc2l9CtRqF"},"execution_count":null,"outputs":[]}]}